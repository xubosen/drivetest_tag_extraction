{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0af83a-da80-46b9-a0a8-4e70a3399c00",
   "metadata": {},
   "source": [
    "# Drive Test Tag Generation\n",
    "Generate tags for the written portion of the chinese driving exam using a process similar to BERTopic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf9df9-a4e7-4134-9652-6a522acf2189",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Loading data from a local database into a question bank class."
   ]
  },
  {
   "cell_type": "code",
   "id": "76b55877-b5f6-4901-841d-4187a9511e0d",
   "metadata": {},
   "source": [
    "from src.qb.question_bank import QuestionBank\n",
    "from data_storage.database.json_database import LocalJsonDB\n",
    "\n",
    "db = LocalJsonDB(\"data_storage/database/json_db/data.json\",\n",
    "                 \"data_storage/database/json_db/images\")\n",
    "qb : QuestionBank = db.load()\n",
    "print(qb.question_count())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Format Data\n",
    "Although the Siglip2 model can handle images of different sizes, we will still standardize the image sizes to the same size to avoid unnecessary complications."
   ],
   "id": "7eebd6332bc9e58e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_cleaning.img_reshaper import ImgSquarer\n",
    "\n",
    "IMG_DIR_256 = \"data_cleaning/resized_imgs/img256\"\n",
    "IMG_DIR_512 = \"data_cleaning/resized_imgs/img512\"\n",
    "\n",
    "squarer_256 = ImgSquarer(256)\n",
    "# squarer_512 = ImgSquarer(512)"
   ],
   "id": "5d1b4ef6948dbabb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def resize_images(qb: QuestionBank, squarer: ImgSquarer, new_dir: str) -> None:\n",
    "    for chapter_id in qb.get_all_chapter_num():\n",
    "        for qid in qb.get_qids_by_chapter(chapter_id):\n",
    "            question = qb.get_question(qid)\n",
    "            if question.get_img_path() is not None:\n",
    "                question.set_img_path(squarer.reshape(qid, qb.get_img_dir(), new_dir))"
   ],
   "id": "5e52674cb6084790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "# If the directory is empty, resize images.\n",
    "if not os.listdir(IMG_DIR_256):\n",
    "    print(\"Resizing images to 256x256...\")\n",
    "    resize_images(qb, squarer_256, IMG_DIR_256)\n",
    "else:\n",
    "    print(\"Images already resized to 256x256, skipping...\")"
   ],
   "id": "b4e6d53a53651c93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Create Multimodal Embeddings\n",
    "Create multimodal embeddings for the questions using a Siglip2 model."
   ],
   "id": "9107061d51b64c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Library Imports\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "# Local Imports\n",
    "from embedder.siglip2_qb_embedder import Siglip2QBEmbedder"
   ],
   "id": "21c02234f40c68ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### a) Load/Download the Siglip2 Model\n",
    "We will be using \"google/siglip2-base-patch16-256\" for this task."
   ],
   "id": "1aec01c979cca317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "MODEL_NAME = \"google/siglip2-base-patch16-256\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, attn_implementation=\"sdpa\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, use_fast=True) # Ensure the model is on the correct device"
   ],
   "id": "801a62831dd141bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### b) Create embeddings",
   "id": "68e3e08cc3a642eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### i) Define a logger",
   "id": "601cfc0d981abcf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from logging import Logger\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "LOGGING_PATH = \"logs\"\n",
    "\n",
    "def get_logger(name: str) -> Logger:\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO) # Set the logging level\n",
    "\n",
    "    # Create a file handler with timestamp in filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    file_handler = logging.FileHandler(\n",
    "        os.path.join(LOGGING_PATH, f\"{name}_{timestamp}.log\")\n",
    "    )\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handler to logger\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "embedder_logger = get_logger(\"embedder\")"
   ],
   "id": "c3d92e8a645cba3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ii) Create the embedder",
   "id": "3b4d7e4f38ef5344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "custom_embedder = Siglip2QBEmbedder(model, processor, embedder_logger)",
   "id": "7b3276b9dc473fd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### iii) Generate embeddings",
   "id": "a6c1d319ac524873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EMBEDDINGS_DIR = \"data_storage/embedding_dir\"\n",
    "EMBEDDING_FILE_NAME = \"siglip2_embeddings.npz\"\n",
    "\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "embedding_file = os.path.join(EMBEDDINGS_DIR, EMBEDDING_FILE_NAME)\n",
    "print(embedding_file)"
   ],
   "id": "9b16ff69a4ddfd84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if EMBEDDING_FILE_NAME in os.listdir(EMBEDDINGS_DIR):\n",
    "    print(f\"Embeddings already exist at {embedding_file}, skipping generation.\")\n",
    "else:\n",
    "    print(\"Generating embeddings...\")\n",
    "    # Generate embeddings for the question bank\n",
    "    embeddings = custom_embedder.encode_qb(qb)"
   ],
   "id": "73393130d3b64fcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### iv) Save embeddings",
   "id": "5216673cda9d198c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_embeddings(embeddings, file_path):\n",
    "    np.savez(file_path, **{str(qid): embeddings[qid] for qid in embeddings})\n",
    "\n",
    "if not os.path.exists(embedding_file):\n",
    "    print(f\"Saving embeddings to {embedding_file}...\")\n",
    "    save_embeddings(embeddings, embedding_file)\n",
    "else:\n",
    "    print(f\"Embeddings file {embedding_file} already exists, skipping save.\")"
   ],
   "id": "32a35483496c3770",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Dimension Reduction",
   "id": "f8d344e935d31ec6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a) Load Embeddings",
   "id": "4dd168213eb8a39b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To load the embeddings later:\n",
    "def load_embeddings(file_path):\n",
    "    loaded = np.load(file_path)\n",
    "    return {key: loaded[key] for key in loaded.files}\n",
    "id_to_embedding = load_embeddings(embedding_file)"
   ],
   "id": "350c58c98f0e98cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from embedder.siglip2_qb_embedder import format_question\n",
    "\n",
    "def format_for_clustering(id_to_embedding: dict, qb: QuestionBank) -> (List[str], List[str], np.ndarray):\n",
    "    \"\"\"\n",
    "    Format the embeddings for clustering.\n",
    "    \"\"\"\n",
    "    qid_lst = []\n",
    "    documents: List[str] = []\n",
    "    embedding_lst: List[np.ndarray] = []\n",
    "    for chapter_id in qb.get_all_chapter_num():\n",
    "        for qid in qb.get_qids_by_chapter(chapter_id):\n",
    "            doc = format_question(qb.get_question(qid), qb.describe_chapter(chapter_id))\n",
    "            qid_lst.append(qid)\n",
    "            documents.append(doc)\n",
    "            embedding_lst.append(id_to_embedding[qid])\n",
    "    embedding_array = np.array(embedding_lst)\n",
    "    return qid_lst, documents, embedding_array"
   ],
   "id": "d9469e155cb15bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qid_lst, documents, embeds = format_for_clustering(id_to_embedding, qb)\n",
    "print(f\"Number of questions: {len(qid_lst)} \"\n",
    "      f\"Embedding shape: {embeds.shape}\")"
   ],
   "id": "158b6357d70c9e6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "documents[:5]  # Display the first 5 documents to verify formatting",
   "id": "6fe21b3805ae0bce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### b) Set up dimension reduction model\n",
    "We will be using UMAP for dimension reduction."
   ],
   "id": "1fbf3eb18a15ac7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "def dim_reduction(dimensions: int, n_neighbors: int) -> np.ndarray:\n",
    "    umap_model = UMAP(\n",
    "        n_components=dimensions,\n",
    "        metric='cosine',\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=0.0\n",
    "    )\n",
    "    return umap_model.fit_transform(embeds)"
   ],
   "id": "f8f78fce0d067b1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### c) Experiment with different parameters\n",
    "Using dimension 2, experiment with different values of `n_neighbors` to see how it affects the clustering."
   ],
   "id": "83ed93a2d97a6a01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_rdc_embeddings(embeddings, file_path):\n",
    "    np.savez(file_path, embeddings=embeddings)"
   ],
   "id": "41bdc630c3f51192",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_rdc_embeddings(file_path):\n",
    "    loaded = np.load(file_path)\n",
    "    return loaded['embeddings']"
   ],
   "id": "aef5c918136496ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "def make_test_embeds() -> Dict[int, np.ndarray]:\n",
    "    n_to_embed = {}\n",
    "    for n in range(5, 30, 5):\n",
    "        file_name = f\"data_storage/embedding_dir/2d_rdc_embeds_n{n}.npz\"\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Reducing dimensions with n_neighbors={n}...\")\n",
    "            rdc_embeds = dim_reduction(2, n)\n",
    "            n_to_embed[n] = rdc_embeds\n",
    "            print(f\"Saving reduced dimension embeddings to {file_name}...\")\n",
    "            save_rdc_embeddings(rdc_embeds, file_name)\n",
    "        else:\n",
    "            print(f\"Reduced dimension embeddings file {file_name} already exists. Loading embedding.\")\n",
    "            n_to_embed[n] = load_rdc_embeddings(file_name)\n",
    "    return n_to_embed"
   ],
   "id": "5989827570459b6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize the embeddings using seaborn",
   "id": "4f4fb49c3ef400fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "def plot_2d_embeds(embeds_2d, title=None):\n",
    "    \"\"\"\n",
    "    Plot a single 2D embedding using seaborn.\n",
    "\n",
    "    Args:\n",
    "        embeds_2d: 2D embeddings array with shape (n_samples, 2)\n",
    "        title: Optional title for the plot\n",
    "\n",
    "    Returns:\n",
    "        The scatter plot object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embeds_2d[:, 0],\n",
    "        y=embeds_2d[:, 1],\n",
    "        alpha=0.7,\n",
    "        s=40  # Point size\n",
    "    )\n",
    "    if title:\n",
    "        plt.title(title, fontsize=15)\n",
    "    else:\n",
    "        plt.title('2D Embedding Visualization', fontsize=15)\n",
    "    plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "    plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Remove top and right borders\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return scatter\n",
    "\n",
    "def plot_multiple_2d_embeds(n_to_embeds):\n",
    "    \"\"\"\n",
    "    Plot multiple 2D embeddings with different n_neighbors values.\n",
    "\n",
    "    Args:\n",
    "        n_to_embeds: Dictionary mapping n_neighbors values to 2D embeddings\n",
    "\n",
    "    Returns:\n",
    "        The matplotlib figure containing all subplots\n",
    "    \"\"\"\n",
    "    # Calculate grid dimensions based on number of embeddings\n",
    "    n_plots = len(n_to_embeds)\n",
    "    n_cols = min(3, n_plots)  # Maximum 3 columns\n",
    "    n_rows = math.ceil(n_plots / n_cols)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "\n",
    "    # Flatten axes if it's a 2D array\n",
    "    if n_plots > 1:\n",
    "        if n_rows > 1 and n_cols > 1:\n",
    "            axes = axes.flatten()\n",
    "        elif n_rows == 1:\n",
    "            axes = [axes[i] for i in range(n_cols)]\n",
    "        elif n_cols == 1:\n",
    "            axes = [axes[i] for i in range(n_rows)]\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Sort n values for consistent visualization\n",
    "    sorted_n_values = sorted(n_to_embeds.keys())\n",
    "\n",
    "    # Plot each embedding\n",
    "    for i, n in enumerate(sorted_n_values):\n",
    "        embeds = n_to_embeds[n]\n",
    "        ax = axes[i]\n",
    "\n",
    "        sns.scatterplot(\n",
    "            x=embeds[:, 0],\n",
    "            y=embeds[:, 1],\n",
    "            alpha=0.7,\n",
    "            s=30,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        ax.set_title(f'n_neighbors = {n}', fontsize=14)\n",
    "        ax.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "        ax.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "        ax.grid(alpha=0.3)\n",
    "        sns.despine(ax=ax)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ],
   "id": "dbdd049e76324dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EXPERIMENT_N_NEIGHBORS = False\n",
    "if EXPERIMENT_N_NEIGHBORS:\n",
    "    # Visualize the embeddings for different n_neighbors values\n",
    "    n_to_embeds = make_test_embeds()\n",
    "    fig = plot_multiple_2d_embeds(n_to_embeds)\n",
    "    plt.show()"
   ],
   "id": "763e5951aa70e72f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### d) Using an n_neighbors value, create high dimensional embeddings\n",
    "We will be using n_neighbours = 10 and dimensions = 256 for the final embeddings."
   ],
   "id": "4f116f0a84db7ebf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_neighbors = 10\n",
    "dim = 256\n",
    "rdc_embeds_path = f\"data_storage/embedding_dir/rdc_embeds_n{n_neighbors}d{dim}.npz\"\n",
    "if not os.path.exists(rdc_embeds_path):\n",
    "    print(f\"Reducing dimensions to {dim} with n_neighbors={n_neighbors}...\")\n",
    "    rdc_embeds = dim_reduction(dim, n_neighbors)\n",
    "    print(f\"Saving reduced dimension embeddings to {rdc_embeds_path}...\")\n",
    "    save_rdc_embeddings(rdc_embeds, rdc_embeds_path)\n",
    "else:\n",
    "    print(f\"Reduced dimension embeddings file {rdc_embeds_path} already exists. Loading embedding.\")\n",
    "    rdc_embeds = load_rdc_embeddings(rdc_embeds_path)\n",
    "print(f\"Embeddings successfully loaded. Shape: {rdc_embeds.shape}\")"
   ],
   "id": "ff87b3f1edfc8670",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Clustering\n",
   "id": "345e9ec83eeca528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate a small representative sample of the question bank by clustering the questions and selecting representative questions from each cluster.",
   "id": "58bd3bc4c40872a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "def cluster_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster the embeddings using HDBSCAN.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Embeddings array with shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        Cluster labels for each embedding\n",
    "    \"\"\"\n",
    "    clusterer = HDBSCAN(min_cluster_size=4, metric='cosine', min_samples=2, cluster_selection_method=\"leaf\", cluster_selection_epsilon=0.00000001)\n",
    "    cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    return cluster_labels"
   ],
   "id": "684713f810028a25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cluster_labels = cluster_embeddings(rdc_embeds)",
   "id": "6d5611d5f3ae22eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "topic_df = DataFrame({\"topic\": cluster_labels, \"id\": qid_lst, \"document\": documents})\n",
    "topic_df.head()"
   ],
   "id": "3fca49d9113e2e74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic_df_sorted = topic_df.sort_values(\"topic\", ascending=True)[topic_df[\"topic\"] != -1]\n",
    "topic_df_sorted.head(100)"
   ],
   "id": "de3ba57bfc27ae01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "15433ca63c822add",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drivetest_tag_gen",
   "language": "python",
   "name": "drivetest_tag_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
