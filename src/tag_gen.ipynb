{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0af83a-da80-46b9-a0a8-4e70a3399c00",
   "metadata": {},
   "source": [
    "# Drive Test Tag Generation\n",
    "Generate tags for the written portion of the chinese driving exam using a process similar to BERTopic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf9df9-a4e7-4134-9652-6a522acf2189",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Loading data from a local database into a question bank class."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:12.633844Z",
     "start_time": "2025-07-08T12:05:12.625993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.qb.question_bank import QuestionBank\n",
    "from data_storage.database.json_database import LocalJsonDB"
   ],
   "id": "a9b70a7d29109633",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "76b55877-b5f6-4901-841d-4187a9511e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:12.950743Z",
     "start_time": "2025-07-08T12:05:12.809416Z"
    }
   },
   "source": [
    "db = LocalJsonDB(\"data_storage/database/json_db/data.json\",\n",
    "                 \"data_storage/database/json_db/images\")\n",
    "qb : QuestionBank = db.load()\n",
    "print(qb.question_count())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2836\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Format Data\n",
    "Although the Siglip2 model can handle images of different sizes, we will still standardize the image sizes to the same size to avoid unnecessary complications."
   ],
   "id": "7eebd6332bc9e58e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:12.992390Z",
     "start_time": "2025-07-08T12:05:12.968336Z"
    }
   },
   "cell_type": "code",
   "source": "from data_cleaning.img_reshaper import ImgSquarer",
   "id": "2a7bdf10bb9554ea",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:13.019702Z",
     "start_time": "2025-07-08T12:05:13.017026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMG_DIR_256 = \"data_cleaning/resized_imgs/img256\"\n",
    "squarer_256 = ImgSquarer(256)"
   ],
   "id": "5d1b4ef6948dbabb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:13.044655Z",
     "start_time": "2025-07-08T12:05:13.041803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resize_images(qb: QuestionBank, squarer: ImgSquarer, new_dir: str) -> None:\n",
    "    for chapter_id in qb.get_all_chapter_num():\n",
    "        for qid in qb.get_qids_by_chapter(chapter_id):\n",
    "            question = qb.get_question(qid)\n",
    "            if question.get_img_path() is not None:\n",
    "                question.set_img_path(squarer.reshape(qid, qb.get_img_dir(), new_dir))"
   ],
   "id": "5e52674cb6084790",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:13.073885Z",
     "start_time": "2025-07-08T12:05:13.071578Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "id": "710dd459b356977b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:13.116105Z",
     "start_time": "2025-07-08T12:05:13.108386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If the directory is empty, resize images.\n",
    "if not os.listdir(IMG_DIR_256):\n",
    "    print(\"Resizing images to 256x256...\")\n",
    "    resize_images(qb, squarer_256, IMG_DIR_256)\n",
    "else:\n",
    "    print(\"Images already resized to 256x256, skipping...\")"
   ],
   "id": "b4e6d53a53651c93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images already resized to 256x256, skipping...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Create Multimodal Embeddings\n",
    "Create multimodal embeddings for the questions using a Siglip2 model."
   ],
   "id": "9107061d51b64c55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:20.820894Z",
     "start_time": "2025-07-08T12:05:13.159724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Library Imports\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "# Local Imports\n",
    "from embedder.siglip2_qb_embedder import Siglip2QBEmbedder"
   ],
   "id": "21c02234f40c68ef",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### a) Load/Download the Siglip2 Model\n",
    "We will be using \"google/siglip2-base-patch16-256\" for this task."
   ],
   "id": "1aec01c979cca317"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:20.834731Z",
     "start_time": "2025-07-08T12:05:20.832560Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "e7936f11c52f6e3e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:33.755250Z",
     "start_time": "2025-07-08T12:05:20.848840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"google/siglip2-base-patch16-256\"\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, attn_implementation=\"sdpa\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, use_fast=True)"
   ],
   "id": "801a62831dd141bf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### b) Create embeddings",
   "id": "68e3e08cc3a642eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### i) Define a logger",
   "id": "601cfc0d981abcf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:35.247568Z",
     "start_time": "2025-07-08T12:05:35.242277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from logging import Logger\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "LOGGING_PATH = \"logs\"\n",
    "\n",
    "def get_logger(name: str) -> Logger:\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO) # Set the logging level\n",
    "\n",
    "    # Create a file handler with timestamp in filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    file_handler = logging.FileHandler(\n",
    "        os.path.join(LOGGING_PATH, f\"{name}_{timestamp}.log\")\n",
    "    )\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handler to logger\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "embedder_logger = get_logger(\"embedder\")"
   ],
   "id": "c3d92e8a645cba3a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ii) Create the embedder",
   "id": "3b4d7e4f38ef5344"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:36.784431Z",
     "start_time": "2025-07-08T12:05:36.781264Z"
    }
   },
   "cell_type": "code",
   "source": "custom_embedder = Siglip2QBEmbedder(model, processor, embedder_logger)",
   "id": "7b3276b9dc473fd6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### iii) Generate embeddings",
   "id": "a6c1d319ac524873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:38.332519Z",
     "start_time": "2025-07-08T12:05:38.329904Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "3223e1f5b6d0482f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:39.824351Z",
     "start_time": "2025-07-08T12:05:39.821347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_embeddings(embeddings, file_path):\n",
    "    np.savez(file_path, **{str(qid): embeddings[qid] for qid in embeddings})"
   ],
   "id": "fa0458e4d525756a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:41.304895Z",
     "start_time": "2025-07-08T12:05:41.301173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBEDDING_FILE_NAME = \"data_storage/embedding_dir/siglip2_embeddings.npz\"\n",
    "if os.path.exists(EMBEDDING_FILE_NAME):\n",
    "    print(f\"Embeddings already exist at {EMBEDDING_FILE_NAME}, skipping generation.\")\n",
    "else:\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = custom_embedder.encode_qb(qb)\n",
    "\n",
    "    print(f\"Saving embeddings to {EMBEDDING_FILE_NAME}...\")\n",
    "    save_embeddings(embeddings, EMBEDDING_FILE_NAME)"
   ],
   "id": "73393130d3b64fcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already exist at data_storage/embedding_dir/siglip2_embeddings.npz, skipping generation.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Dimension Reduction",
   "id": "f8d344e935d31ec6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a) Load Embeddings",
   "id": "4dd168213eb8a39b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:42.749808Z",
     "start_time": "2025-07-08T12:05:42.746400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_embeddings(file_path):\n",
    "    loaded = np.load(file_path)\n",
    "    return {key: loaded[key] for key in loaded.files}"
   ],
   "id": "c521938a9f801a23",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:44.702999Z",
     "start_time": "2025-07-08T12:05:44.212737Z"
    }
   },
   "cell_type": "code",
   "source": "id_to_embedding = load_embeddings(EMBEDDING_FILE_NAME)",
   "id": "350c58c98f0e98cb",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:45.941025Z",
     "start_time": "2025-07-08T12:05:45.937603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from embedder.siglip2_qb_embedder import format_question"
   ],
   "id": "4917a450667c3f9f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:47.215073Z",
     "start_time": "2025-07-08T12:05:47.210109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_for_clustering(id_to_embedding: dict, qb: QuestionBank) -> (List[str], List[str], np.ndarray):\n",
    "    \"\"\"\n",
    "    Format the embeddings for clustering.\n",
    "    \"\"\"\n",
    "    qid_lst: List[str] = []\n",
    "    documents: List[str] = []\n",
    "    embedding_lst: List[np.ndarray] = []\n",
    "\n",
    "    for chapter_id in qb.get_all_chapter_num():\n",
    "        for qid in qb.get_qids_by_chapter(chapter_id):\n",
    "\n",
    "            doc = format_question(qb.get_question(qid),\n",
    "                                  qb.describe_chapter(chapter_id))\n",
    "\n",
    "            qid_lst.append(qid)\n",
    "            documents.append(doc)\n",
    "            embedding_lst.append(id_to_embedding[qid])\n",
    "\n",
    "    embedding_array = np.array(embedding_lst)\n",
    "    return qid_lst, documents, embedding_array"
   ],
   "id": "d9469e155cb15bbb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:48.578282Z",
     "start_time": "2025-07-08T12:05:48.568977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qid_lst, documents, embeds = format_for_clustering(id_to_embedding, qb)\n",
    "print(f\"Number of questions: {len(qid_lst)} \"\n",
    "      f\"Embedding shape: {embeds.shape}\")"
   ],
   "id": "158b6357d70c9e6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 2836 Embedding shape: (2836, 768)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:49.841741Z",
     "start_time": "2025-07-08T12:05:49.835817Z"
    }
   },
   "cell_type": "code",
   "source": "documents[:2]  # Display the first 5 documents to verify formatting",
   "id": "6fe21b3805ae0bce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['题目:驾驶机动车遇到沙尘、冰雹、雨、雾、结冰等气象条件如何行驶？答案:降低行驶速度', '题目:驾驶机动车时可以向道路上抛撒物品。答案:错']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### b) Set up dimension reduction model\n",
    "We will be using UMAP for dimension reduction."
   ],
   "id": "1fbf3eb18a15ac7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:55.286518Z",
     "start_time": "2025-07-08T12:05:51.197195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "def dim_reduction(dimensions: int, n_neighbors: int, embeddings: np.ndarray) -> np.ndarray:\n",
    "    umap_model = UMAP(\n",
    "        n_components=dimensions,\n",
    "        metric='cosine',\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=0.0\n",
    "    )\n",
    "    return umap_model.fit_transform(embeddings)"
   ],
   "id": "f8f78fce0d067b1b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:56.724524Z",
     "start_time": "2025-07-08T12:05:56.721411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_rdc_embeddings(embeddings, file_path):\n",
    "    np.savez(file_path, embeddings=embeddings)"
   ],
   "id": "41bdc630c3f51192",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:58.193234Z",
     "start_time": "2025-07-08T12:05:58.190235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_rdc_embeddings(file_path):\n",
    "    loaded = np.load(file_path)\n",
    "    return loaded['embeddings']"
   ],
   "id": "aef5c918136496ec",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:05:59.637660Z",
     "start_time": "2025-07-08T12:05:59.634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_reduced_embeddings(dimension, n_neighbors, embeddings):\n",
    "    path = f\"data_storage/embedding_dir/rdc_embeds_n{n_neighbors}d{dimension}.npz\"\n",
    "    if not os.path.exists(path):\n",
    "        rdc_embeds = dim_reduction(dimension, n_neighbors, embeddings)\n",
    "        save_rdc_embeddings(rdc_embeds, path)\n",
    "    else:\n",
    "        rdc_embeds = load_rdc_embeddings(path)\n",
    "    return rdc_embeds"
   ],
   "id": "5989827570459b6e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Clustering\n",
   "id": "345e9ec83eeca528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate a small representative sample of the question bank by clustering the questions and selecting representative questions from each cluster.",
   "id": "58bd3bc4c40872a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a) Set up clustering model",
   "id": "c952c674ec461cf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:06:01.221843Z",
     "start_time": "2025-07-08T12:06:01.103116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "def cluster_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster the embeddings using HDBSCAN.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Embeddings array with shape (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        Cluster labels for each embedding\n",
    "    \"\"\"\n",
    "    clusterer = HDBSCAN(min_cluster_size=2, metric='cosine', min_samples=1, cluster_selection_method=\"leaf\", allow_single_cluster=True)\n",
    "    return clusterer.fit_predict(embeddings)"
   ],
   "id": "684713f810028a25",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### b) Tune Hyperparameters\n",
    "Find optimal hyperparameters\n",
    "#### i) Set up experiment trying a range of hyperparameters"
   ],
   "id": "49c13f6845e6018"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:06:02.676930Z",
     "start_time": "2025-07-08T12:06:02.674462Z"
    }
   },
   "cell_type": "code",
   "source": "from pandas import DataFrame",
   "id": "e272fa7d8af0f63e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:06:04.138250Z",
     "start_time": "2025-07-08T12:06:04.134526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_topics(dimension: int, n_neighbors: int, qid_lst: List[str], docs: List[str], embeddings: np.ndarray) -> DataFrame:\n",
    "    \"\"\"\n",
    "    A pipeline that takes in the dimension and n_neighbors, then performs dimension reduction and clustering using those hyperparameters on docs.\n",
    "    \"\"\"\n",
    "    topic_labels = cluster_embeddings(make_reduced_embeddings(dimension, n_neighbors, embeddings))\n",
    "    return DataFrame({\n",
    "        \"topic\" : topic_labels,\n",
    "        \"qid\": qid_lst,\n",
    "        \"question\": docs\n",
    "    })"
   ],
   "id": "a4b8bb83b92c6db2",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:06:05.619964Z",
     "start_time": "2025-07-08T12:06:05.616962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "from ipywidgets import IntProgress, Label, HBox\n",
    "from IPython.display import display"
   ],
   "id": "7d7ef6d5d0fdb08b",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:06:07.074557Z",
     "start_time": "2025-07-08T12:06:07.069967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dimred_experiment(qid_lst: List[str], questions: List[str], embeddings: np.ndarray, n_d_pairs: List[Tuple[int]]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    An experiment to find the optimal hyperparameters for dimension reduction (dimensions and n_neighbors).\n",
    "\n",
    "    With a fixed clustering method, we will vary the dimensions and n_neighbors to see how it affects the outlier count.\n",
    "    \"\"\"\n",
    "\n",
    "    cur, total = 0, len(n_d_pairs)\n",
    "    progress_bar = IntProgress(min=0, max=total, description='Progress:')\n",
    "    progress_label = Label(f'0/{total} complete')\n",
    "    progress_box = HBox([progress_bar, progress_label])\n",
    "    display(progress_box)\n",
    "\n",
    "    result_dict = {\n",
    "        \"qid\": qid_lst,\n",
    "        \"question\": questions\n",
    "    }\n",
    "    for n_neighbors, dimension in n_d_pairs:\n",
    "        progress_label.value = f\"{cur}/{total} complete. Calculating n = {n_neighbors}, d = {dimension}...\"\n",
    "\n",
    "        column_name = f\"n{n_neighbors}d{dimension}\"\n",
    "        result_dict[column_name] = cluster_embeddings(make_reduced_embeddings(dimension, n_neighbors, embeddings))\n",
    "\n",
    "        cur += 1\n",
    "        progress_bar.value += 1\n",
    "    progress_label.value = \"Experiment complete!\"\n",
    "    return DataFrame(result_dict)"
   ],
   "id": "436352498f8ee0b2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:06:08.585278Z",
     "start_time": "2025-07-08T12:06:08.582613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import warnings"
   ],
   "id": "39346c793efa1c08",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T12:51:20.352638Z",
     "start_time": "2025-07-08T12:06:10.042412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DIMRED_EXPR1_PATH = \"data_storage/experiments/dimension_reduction_experiment1.csv\"\n",
    "if not os.path.exists(DIMRED_EXPR1_PATH):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress all warnings within this block\n",
    "        n_d_pairs = [(n, d) for n in range(2, 767, 50) for d in range(2, 767, 50)]\n",
    "        results = dimred_experiment(qid_lst=qid_lst,\n",
    "                                    questions=documents,\n",
    "                                    embeddings=embeds,\n",
    "                                    n_d_pairs=n_d_pairs)\n",
    "        results.to_csv(DIMRED_EXPR1_PATH, index=False)\n",
    "        dimred_exp1_results = results\n",
    "else:\n",
    "    print(f\"Results already exist at {DIMRED_EXPR1_PATH}. Loading results from file...\")\n",
    "    dimred_exp1_results = pd.read_csv(DIMRED_EXPR1_PATH)"
   ],
   "id": "214d8889d50872e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntProgress(value=0, max=256)"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7b4b472cac440dc8bed70356543af67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m warnings.simplefilter(\u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# Suppress all warnings within this block\u001B[39;00m\n\u001B[32m      5\u001B[39m n_d_pairs = [(n, d) \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m2\u001B[39m, \u001B[32m767\u001B[39m, \u001B[32m50\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m2\u001B[39m, \u001B[32m767\u001B[39m, \u001B[32m50\u001B[39m)]\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m results = dimred_experiment(qid_lst=qid_lst,\n\u001B[32m      7\u001B[39m                             questions=documents,\n\u001B[32m      8\u001B[39m                             embeddings=embeds,\n\u001B[32m      9\u001B[39m                             n_d_pairs=n_d_pairs)\n\u001B[32m     10\u001B[39m results.to_csv(DIMRED_EXPR1_PATH, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     11\u001B[39m dimred_exp1_results = results\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mdimred_experiment\u001B[39m\u001B[34m(qid_lst, questions, embeddings, n_d_pairs)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m n_neighbors, dimension \u001B[38;5;129;01min\u001B[39;00m n_d_pairs:\n\u001B[32m     16\u001B[39m     column_name = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_neighbors\u001B[38;5;132;01m}\u001B[39;00m\u001B[33md\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdimension\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     result_dict[column_name] = cluster_embeddings(make_reduced_embeddings(dimension, n_neighbors, embeddings))\n\u001B[32m     18\u001B[39m     progress_bar.value += \u001B[32m1\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(result_dict)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36mmake_reduced_embeddings\u001B[39m\u001B[34m(dimension, n_neighbors, embeddings)\u001B[39m\n\u001B[32m      2\u001B[39m path = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdata_storage/embedding_dir/rdc_embeds_n\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_neighbors\u001B[38;5;132;01m}\u001B[39;00m\u001B[33md\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdimension\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.npz\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(path):\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     rdc_embeds = dim_reduction(dimension, n_neighbors, embeddings)\n\u001B[32m      5\u001B[39m     save_rdc_embeddings(rdc_embeds, path)\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mdim_reduction\u001B[39m\u001B[34m(dimensions, n_neighbors, embeddings)\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdim_reduction\u001B[39m(dimensions: \u001B[38;5;28mint\u001B[39m, n_neighbors: \u001B[38;5;28mint\u001B[39m, embeddings: np.ndarray) -> np.ndarray:\n\u001B[32m      3\u001B[39m     umap_model = UMAP(\n\u001B[32m      4\u001B[39m         n_components=dimensions,\n\u001B[32m      5\u001B[39m         metric=\u001B[33m'\u001B[39m\u001B[33mcosine\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      6\u001B[39m         n_neighbors=n_neighbors,\n\u001B[32m      7\u001B[39m         min_dist=\u001B[32m0.0\u001B[39m\n\u001B[32m      8\u001B[39m     )\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m umap_model.fit_transform(embeddings)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/umap/umap_.py:2928\u001B[39m, in \u001B[36mUMAP.fit_transform\u001B[39m\u001B[34m(self, X, y, force_all_finite, **kwargs)\u001B[39m\n\u001B[32m   2890\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y=\u001B[38;5;28;01mNone\u001B[39;00m, force_all_finite=\u001B[38;5;28;01mTrue\u001B[39;00m, **kwargs):\n\u001B[32m   2891\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Fit X into an embedded space and return that transformed\u001B[39;00m\n\u001B[32m   2892\u001B[39m \u001B[33;03m    output.\u001B[39;00m\n\u001B[32m   2893\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   2926\u001B[39m \u001B[33;03m        Local radii of data points in the embedding (log-transformed).\u001B[39;00m\n\u001B[32m   2927\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2928\u001B[39m     \u001B[38;5;28mself\u001B[39m.fit(X, y, force_all_finite, **kwargs)\n\u001B[32m   2929\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform_mode == \u001B[33m\"\u001B[39m\u001B[33membedding\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   2930\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.output_dens:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/umap/umap_.py:2555\u001B[39m, in \u001B[36mUMAP.fit\u001B[39m\u001B[34m(self, X, y, force_all_finite, **kwargs)\u001B[39m\n\u001B[32m   2552\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   2553\u001B[39m     \u001B[38;5;66;03m# sklearn pairwise_distances fails for callable metric on sparse data\u001B[39;00m\n\u001B[32m   2554\u001B[39m     _m = \u001B[38;5;28mself\u001B[39m.metric \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sparse_data \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._input_distance_func\n\u001B[32m-> \u001B[39m\u001B[32m2555\u001B[39m     dmat = pairwise_distances(X[index], metric=_m, **\u001B[38;5;28mself\u001B[39m._metric_kwds)\n\u001B[32m   2556\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mValueError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   2557\u001B[39m     \u001B[38;5;66;03m# metric is numba.jit'd or not supported by sklearn,\u001B[39;00m\n\u001B[32m   2558\u001B[39m     \u001B[38;5;66;03m# fallback to pairwise special\u001B[39;00m\n\u001B[32m   2560\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sparse_data:\n\u001B[32m   2561\u001B[39m         \u001B[38;5;66;03m# Get a fresh metric since we are casting to dense\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:218\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    224\u001B[39m     msg = re.sub(\n\u001B[32m    225\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    226\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    228\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2477\u001B[39m, in \u001B[36mpairwise_distances\u001B[39m\u001B[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001B[39m\n\u001B[32m   2474\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001B[32m   2475\u001B[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001B[32m-> \u001B[39m\u001B[32m2477\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _parallel_pairwise(X, Y, func, n_jobs, **kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1960\u001B[39m, in \u001B[36m_parallel_pairwise\u001B[39m\u001B[34m(X, Y, func, n_jobs, **kwds)\u001B[39m\n\u001B[32m   1957\u001B[39m X, Y, dtype = _return_float_dtype(X, Y)\n\u001B[32m   1959\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m effective_n_jobs(n_jobs) == \u001B[32m1\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m func(X, Y, **kwds)\n\u001B[32m   1962\u001B[39m \u001B[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001B[39;00m\n\u001B[32m   1963\u001B[39m fd = delayed(_dist_wrapper)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1996\u001B[39m, in \u001B[36m_pairwise_callable\u001B[39m\u001B[34m(X, Y, metric, ensure_all_finite, **kwds)\u001B[39m\n\u001B[32m   1992\u001B[39m iterator = itertools.combinations(\u001B[38;5;28mrange\u001B[39m(X.shape[\u001B[32m0\u001B[39m]), \u001B[32m2\u001B[39m)\n\u001B[32m   1993\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, j \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[32m   1994\u001B[39m     \u001B[38;5;66;03m# scipy has not yet implemented 1D sparse slices; once implemented this can\u001B[39;00m\n\u001B[32m   1995\u001B[39m     \u001B[38;5;66;03m# be removed and `arr[ind]` can be simply used.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1996\u001B[39m     x = X[[i], :] \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;28;01melse\u001B[39;00m X[i]\n\u001B[32m   1997\u001B[39m     y = Y[[j], :] \u001B[38;5;28;01mif\u001B[39;00m issparse(Y) \u001B[38;5;28;01melse\u001B[39;00m Y[j]\n\u001B[32m   1998\u001B[39m     out[i, j] = metric(x, y, **kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/site-packages/scipy/_lib/_sparse.py:41\u001B[39m, in \u001B[36missparse\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34missparse\u001B[39m(x):\n\u001B[32m     11\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001B[39;00m\n\u001B[32m     12\u001B[39m \n\u001B[32m     13\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     39\u001B[39m \u001B[33;03m    False\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, SparseABC)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/opt/anaconda3/envs/drivetest_tag_gen/lib/python3.11/abc.py:119\u001B[39m, in \u001B[36mABCMeta.__instancecheck__\u001B[39m\u001B[34m(cls, instance)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__instancecheck__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, instance):\n\u001B[32m    118\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _abc_instancecheck(\u001B[38;5;28mcls\u001B[39m, instance)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dimred_exp1_results.shape",
   "id": "ac482d94f03281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ii) Analyze Results",
   "id": "c24ce1eaf80a83bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract key metrics from the data into a dataframe.",
   "id": "7754d0f948c5f4ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_n_d(col_name: str) -> (int, int):\n",
    "    \"\"\"\n",
    "    Extract n_neighbors and dimensions from the column name.\n",
    "    \"\"\"\n",
    "    parts = col_name.split(\"d\")\n",
    "    n_neighbors = int(parts[0][1:])  # Remove 'n' and convert to int\n",
    "    dimensions = int(parts[1])  # Convert to int\n",
    "    return n_neighbors, dimensions"
   ],
   "id": "515f22e2b9aa6f36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_dimred_data(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Set up a data frame with n_neighbours and dimensions as the first 2 columns, followed by\n",
    "    key metrics, such as:\n",
    "        - Number of clusters\n",
    "        - Number of outliers\n",
    "        - ...\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"n_neighbors\": [],\n",
    "        \"dimensions\": [],\n",
    "        \"num_clusters\": [],\n",
    "        \"num_outliers\": []\n",
    "    }\n",
    "    for col in data.columns:\n",
    "        if col.startswith(\"n\") and \"d\" in col:\n",
    "            n_neighbors, dimensions = extract_n_d(col)\n",
    "            metrics[\"n_neighbors\"].append(n_neighbors)\n",
    "            metrics[\"dimensions\"].append(dimensions)\n",
    "            metrics[\"num_clusters\"].append(data[col].nunique())\n",
    "            metrics[\"num_outliers\"].append((data[col] == -1).sum())\n",
    "    return DataFrame(metrics)"
   ],
   "id": "b622ca51b696b88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "OLD_DATA_DIR = \"data_storage/experiments/old\"\n",
    "def load_old_data() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load old data from the specified directory.\n",
    "    \"\"\"\n",
    "    old_data = []\n",
    "    for file_name in os.listdir(OLD_DATA_DIR):\n",
    "        file_path = os.path.join(OLD_DATA_DIR, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            old_data.append(df)\n",
    "        else:\n",
    "            print(f\"File {file_name} does not exist in {OLD_DATA_DIR}.\")\n",
    "    return pd.concat(old_data, ignore_index=True) if old_data else pd.DataFrame()"
   ],
   "id": "5d65eadfc7e5eadb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combined_results = pd.concat([\n",
    "    dimred_exp1_results.drop(columns=[\"id\", \"question\"]),\n",
    "    load_old_data()\n",
    "], axis=1).reindex()"
   ],
   "id": "e8caeb536853ee76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import plotly.express as px",
   "id": "aec6f91e7d77c53a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def interactive_plot3d(df, z_metric, title=None, size=(1600, 800)):\n",
    "    \"\"\"\n",
    "    Creates an interactive 3D plot using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame with 'n_neighbors', 'dimensions', and z_metric\n",
    "    z_metric : str\n",
    "        Column name to use for z-axis values\n",
    "    title : str, optional\n",
    "        Plot title\n",
    "    size : tuple, optional\n",
    "        Figure size in pixels\n",
    "    \"\"\"\n",
    "    if title is None:\n",
    "        title = f'3D Plot of {z_metric} by n_neighbors and dimensions'\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        df,\n",
    "        x='n_neighbors',\n",
    "        y='dimensions',\n",
    "        z=z_metric,\n",
    "        color=z_metric,\n",
    "        color_continuous_scale='Viridis',\n",
    "        range_color=[1, 800],\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=size[0],\n",
    "        height=size[1],\n",
    "        scene=dict(\n",
    "            xaxis_title='n_neighbors',\n",
    "            yaxis_title='dimensions',\n",
    "            zaxis_title=z_metric\n",
    "        ),\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=z_metric,\n",
    "            len=0.75\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fig"
   ],
   "id": "cb52ed966ee4b6e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Examine number of clusters",
   "id": "161a2e21af5927db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = analyze_dimred_data(combined_results)\n",
    "fig = interactive_plot3d(metrics, z_metric='num_clusters')\n",
    "fig.show()"
   ],
   "id": "99a7b104733a732c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Examine number of outliers",
   "id": "2e4b9ae241eb9018"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = interactive_plot3d(metrics, z_metric='num_outliers')\n",
    "fig.show()"
   ],
   "id": "847a0879d3a8f40a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inspect certain points manually to determine the quality of clusters",
   "id": "caa9920607849964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def inspect_topic(data: DataFrame, n_neighbors: int, dimensions: int, sorted=False, ascending=False) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Inspect the results for a specific n_neighbors and dimensions combination.\n",
    "    \"\"\"\n",
    "    col_name = f\"n{n_neighbors}d{dimensions}\"\n",
    "    if col_name in data.columns:\n",
    "        output = data[[\"qid\", \"question\", col_name]].rename(columns={col_name: \"topic\"})\n",
    "        if sorted:\n",
    "            output = output.sort_values(by=\"topic\", ascending=ascending)\n",
    "    else:\n",
    "        raise ValueError(f\"No results found for n_neighbors={n_neighbors} and dimensions={dimensions}\")\n",
    "    return output"
   ],
   "id": "b9d8fdb66d9c04e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "inspect_topic(data=combined_results, n_neighbors=2, dimensions=2, sorted=True)",
   "id": "91a502ef537fdac6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### iii) Refine Experiment Based on Results\n",
    "##### Experiment 2"
   ],
   "id": "3b17a92072e2c6fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_nd_experiment2() -> List[Tuple[int]]:\n",
    "    \"\"\"\n",
    "    Get the n_neighbors and dimensions pairs for the second experiment.\n",
    "    This is based on the results of the first experiment.\n",
    "    \"\"\"\n",
    "    nd_set = set()\n",
    "    for n in range(7, 200, 15):\n",
    "        for d in range(3, 52, 2):\n",
    "            nd_set.add((n, d))\n",
    "    print(len(nd_set), \"n_neighbors and dimensions pairs to test.\")\n",
    "    return list(nd_set)"
   ],
   "id": "936ba4dad2bbfce7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_experiment(qid_lst: List[str], documents: List[str], embeds: np.ndarray, n_d_pairs: List[Tuple[int]], path: str) -> DataFrame:\n",
    "    \"\"\"Set up experiment.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")  # Suppress all warnings within this block\n",
    "            results = dimred_experiment(qid_lst=qid_lst,\n",
    "                                                    questions=documents,\n",
    "                                                    embeddings=embeds,\n",
    "                                                    n_d_pairs=n_d_pairs)\n",
    "            results.to_csv(path, index=False)\n",
    "    else:\n",
    "        results = pd.read_csv(path)\n",
    "    return results"
   ],
   "id": "e6ad89aeaa1da435"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DIMRED_EXPR2_PATH = \"data_storage/experiments/dimension_reduction_experiment2.csv\"",
   "id": "78c6add6a3b0ec83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dimred_exp2_results = run_experiment(qid_lst=qid_lst, documents=documents, embeds=embeds, n_d_pairs=get_nd_experiment2(), path=DIMRED_EXPR2_PATH)",
   "id": "f09333cfb2b2b04b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "combined_results = pd.concat([\n",
    "    combined_results.drop(columns=[\"id\", \"question\"]),\n",
    "    dimred_exp2_results], axis=1).reindex()\n",
    "combined_results.shape"
   ],
   "id": "efc4608f4cd99fbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "metrics = analyze_dimred_data(combined_results)",
   "id": "ada9280c58c8a545"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = interactive_plot3d(metrics, z_metric='num_clusters')\n",
    "fig.show()"
   ],
   "id": "8c172567ce8fd513"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = interactive_plot3d(metrics, z_metric='num_outliers')\n",
    "fig.show()"
   ],
   "id": "27fb3f7cdc4027a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sorted_result = inspect_topic(combined_results, n_neighbors=15, dimensions=3, sorted=True)\n",
    "sorted_result.head(100)"
   ],
   "id": "2650a6e4a98d835e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Experiment 3",
   "id": "d8565e5b108c8749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_nd_experiment3() -> List[Tuple[int]]:\n",
    "    \"\"\"\n",
    "    Get the n_neighbors and dimensions pairs for the third experiment.\n",
    "    This is based on the results of the first and second experiment.\n",
    "    \"\"\"\n",
    "    nd_set = set()\n",
    "    for n in range(100, 700, 55):\n",
    "        for d in range(4, 767, 55):\n",
    "            nd_set.add((n, d))\n",
    "    return list(nd_set)"
   ],
   "id": "9a391d2f9efa5e3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DIMRED_EXPR3_PATH = \"data_storage/experiments/dimension_reduction_experiment3.csv\"\n",
    "dimred_exp3_results = run_experiment(qid_lst=qid_lst, documents=documents, embeds=embeds, n_d_pairs=get_nd_experiment3(), path=DIMRED_EXPR3_PATH)"
   ],
   "id": "8ac4d51cfd7a55be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combined_results = pd.concat([\n",
    "    combined_results.drop(columns=[\"id\", \"question\"]),\n",
    "    dimred_exp3_results], axis=1).reindex()\n",
    "metrics = analyze_dimred_data(combined_results)"
   ],
   "id": "a90ec538875aae93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = interactive_plot3d(metrics, z_metric='num_clusters')\n",
    "fig.show()"
   ],
   "id": "7d64264fc18348c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = interactive_plot3d(metrics, z_metric='num_outliers')\n",
    "fig.show()"
   ],
   "id": "418bab7762482568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inspected_result = inspect_topic(combined_results, n_neighbors=97, dimensions=3, sorted=True)\n",
    "inspected_result.head(100)"
   ],
   "id": "19bea03264d376b3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drivetest_tag_gen",
   "language": "python",
   "name": "drivetest_tag_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
